{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdfminer.six\n",
      "  Downloading pdfminer.six-20200517-py3-none-any.whl (5.6 MB)\n",
      "Requirement already satisfied: chardet; python_version > \"3.0\" in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pdfminer.six) (3.0.4)\n",
      "Collecting pycryptodome\n",
      "  Downloading pycryptodome-3.9.8-cp37-cp37m-win_amd64.whl (14.1 MB)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pdfminer.six) (2.1.0)\n",
      "Installing collected packages: pycryptodome, pdfminer.six\n",
      "Successfully installed pdfminer.six-20200517 pycryptodome-3.9.8\n"
     ]
    }
   ],
   "source": [
    "# !pip install pdfminer.six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import HTMLConverter\n",
    "# from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from io import StringIO\n",
    " \n",
    "path = \"./test.pdf\"\n",
    " \n",
    "rsrcmgr = PDFResourceManager()\n",
    "retstr = StringIO()\n",
    "codec = 'utf-8'\n",
    "laparams = LAParams()\n",
    " \n",
    "f = open('./out.html', 'wb') # html로 바꾼다.\n",
    "device = HTMLConverter(rsrcmgr, f, codec=codec, laparams=laparams)\n",
    " \n",
    "fp = open(path, 'rb')\n",
    "interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "password = \"\"\n",
    "maxpages = 0 #is for all\n",
    "caching = True\n",
    "pagenos=set()\n",
    "for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages, password=password,caching=caching, check_extractable=True):\n",
    "    interpreter.process_page(page)\n",
    "fp.close()\n",
    "device.close()\n",
    "str = retstr.getvalue()\n",
    "retstr.close()\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from html.parser import HTMLParser\n",
    "class MyHTMLParser(HTMLParser):\n",
    "    contents = []\n",
    "    isBody = False\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if 'body' in tag:\n",
    "            self.isBody = True            \n",
    "    \n",
    "    def handle_data(self, data):\n",
    "        if self.isBody:\n",
    "            self.contents.append(data)\n",
    "\n",
    "parser = MyHTMLParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./out.html', encoding='utf8') as xhtml:\n",
    "    parser.feed(''.join(xhtml.readlines())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ''.join(parser.contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nPage 1\\n워드 임베딩과 딥러닝 기법을 이용한 SMS 문자 메시지\\n필터링\\n( SMS Text Messages Filtering using Word Embedding and Deep Learning Techniques )\\n이현영*, 강승식**\\n(Hyun Young Lee, Seung Shik Kang)\\n딥러닝에서 자연어 처리를 위한 텍스트 분석 기법은 워드 임베딩을 통해 단어를 벡터 형태로 표현한다. 본 논문에서는\\n워드 임베딩 기법과 딥러닝 기법을 이용하여 SMS 문자 메시지를 문서 벡터로 구성하고 이를 스팸 문자 메시지와 정상적\\n인 문자 메시지로 분류하는 방법을 제안하였다. 유사한 문맥을 가진 단어들은 벡터 공간에서 인접한 벡터 공간에 표현되\\n도록 하기 위해 전처리 과정으로 자동 띄어쓰기를 적용하고 스팸 문자 메시지로 차단되는 것을 피하기 위한 목적으로 음\\n절의 자모를 특수기호로 왜곡하여 맞춤법이 파괴된 상태로 단어 벡터와 문장 벡터를 생성하였다. 또한 문장 벡터 생성 시\\nCBOW와 skip gram이라는 두 가지 워드 임베딩 알고리즘을 적용하여 문장 벡터를 표현하였으며, 딥러닝을 이용한 스팸\\n문자 메시지 필터링의 성능 평가를 위해 SVM Light와 정확도를 비교 측정하였다.\\n■ 중심어 : 스팸 문자 메시지 ; 워드 임베딩 ; 문장 벡터 ; 딥러닝 ; 이진 분류\\nText analysis technique for natural language processing in deep learning represents words in vector form through\\nword embedding. In this paper, we propose a method of constructing a document vector and classifying it into spam\\nand normal text message, using word embedding and deep learning method.\\nAutomatic spacing applied in the preprocessing process ensures that words with similar context are adjacently\\nrepresented in vector space. Additionally,\\nthe intentional word formation errors with non-alphabetic or\\nextraordinary characters are designed to avoid being blocked by spam message filter.\\nTwo embedding algorithms, CBOW and skip grams, are used to produce the sentence vector and the performance\\nand the accuracy of deep learning based spam filter model are measured by comparing to those of SVM Light.\\n■ keywords :\\nspam text message ; word embedding ; text vector ; deep learning ; binary classification\\nⅠ. 서 론\\n이터를 분류하는 것에 있어서 다른 기법들보다 더 나은 성능을\\n보인다고 알려져 있다[5, 6]. 지지 벡터 기계는 자질 벡터\\n(feature vector)를 이용하는 기계학습 방법으로 자질 벡터의\\n스마트폰이 대중화되면서 SMS(short message service) 문\\n구성이 분류 성능에 큰 영향을 미친다. 이에 따라 스팸 문자 메\\n자 메시지 전송을 통한 의사소통 방식이 보편화됨에 따라 텍스\\n시지 필터링을 위한 벡터 표현은 성능에 영향을 끼치는 중요한\\n트 형태의 데이터의 양도 대량으로 증가하고 있다[1]. 그 부작\\n요소이다[7]. 현재 텍스트에 대한 벡터의 표현 방식으로\\n용으로 성인광고, 대출광고, 게임광고 등의 스팸 문자 텍스트 데\\nTF-IDF가 가장 널리 사용되고 있지만, TF-IDF의 경우에는\\n이터도 폭발적으로 증가하여 이를 필터링 하기 위한 다양한 기\\n단어 수가 증가함에 따라 차원의 수도 같이 증가하여 대용량 단\\n법들이 개발되고 있다[2]. 이러한 스팸 문자 메시지 데이터를\\n어들을 처리하는데 어려움 있다. 이러한 차원수 증가 문제를 해\\n효율적으로 필터링하는 방법 중 널리 보편적으로 사용되는 방\\n결하기 위해 TF-IDF는 고차원의 벡터를 저차원의 벡터로 차\\n법론으로는 통계적 확률 방법론과 지지 벡터 기계(SVM) 등의\\n원 축소 작업을 추가로 수행해야 한다. 하지만 신경망 언어 모\\n기계학습 방법이 있다[3, 4].\\n델의 워드 임베딩 통한 단어 벡터는 기존의 TF-IDF보다 차원\\n기계학습 방법론 중, 지지 벡터 기계는 스팸 문자 메시지 데\\n축소에 자유롭고, 벡터를 통한 단어 의미 유사성 추론에서도 더\\n* 학생회원, 국민대학교 컴퓨터공학학과\\n** 정회원, 국민대학교 소프트웨어학부\\n이 논문은 2017년도 정부(미래창조과학부)의 재원으로 한국연구재단의 지원을 받아 수행된 연구임 (No.NRF-2017M3C4A7068186).\\n접수일자 : 2018년 03월 05일\\n수정일자 : 2018년 04월 11일\\n게재확정일 : 2018년 04월 28일\\n교신저자 : 강승식 e-mail : sskang@kookmin.ac.kr\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPage 2\\n나은 성능을 보여준다[8, 9, 10].\\n수에 비례하게 커지는 차원 수 증가 문제를 해결하였다[13].\\n문서나 문장 등의 데이터를 이미 정해진 범주에 따라 분류\\n그림 1은 단어 벡터를 표현하는 방법인 CBOW와 skip\\n하는 방법으로는 K-NN 방법, SVM, Naive Bayes 등의 기계\\ngram이다. 이 두 개의 워드 임베딩 방법은 서로 다른 방법으\\n학습 방법들을 이용하고 있다[11]. 하지만 스팸 문자 메시지\\n로 단어를 벡터로 표현한다. CBOW의 경우는 중심 단어의 주\\n필터링은 정해진 범주를 2개로 고정하여 여러 문장 및 구 형\\n변 단어들로부터 중심단어를 예측하고, skip gram은 중심단\\n태의 텍스트 데이터를 분류하는 이진 분류(binary\\n어에서 주변 단어들 예측하여 중심 단어를 벡터로 표현한다.\\nclassification) 문제이므로 분류 기법에 적합한 딥러닝 모델을\\n단어 벡터를 생성하는 CBOW와 skip gram으로 문장을 구성\\n이용하여 스팸 문자 메시지를 필터링하고자 하였다.\\n하는 단어 벡터를 생성하여 문장 벡터로 표현하였다.\\n딥러닝 모델 중 분류 문제에 가장 널리 사용되는 모델은 연\\n속성의 데이터를 분류하는 RNN(recurrent neural network),\\n특징 벡터를 추출하여 분류하는 CNN(convolution neural\\nnetwork), 고정 길이의 특징 벡터를 분류하는 전방향 신경망\\n(feed-forward neural network) 등이 있다[12, 13, 14].\\n본 연구에서는 신경망을 이용한 워드 임베딩(word\\nembedding)으로 문자 메시지를 벡터로 표현하고 전방향 신경\\n망을 이용하는 스팸 문자 메시지 필터링 기법을 제안한다.\\nⅡ. 본 론\\n1. 워드 임베딩과 문장 벡터 구성\\nSMS 문자 메시지 데이터를 살펴보면, 하나의 문장 및 구는\\n단어들의 집합이므로 각 단어들의 빈도수를 이용하여 하나의\\n벡터로 표현될 수 있다. 이러한 빈도수 기반의 벡터는 단어의\\n수가 많아짐에 따라 차원의 수도 비례하게 커지는 차원의 저\\n주라는 문제점이 있다. 예를 들어, 아래 5개의 문장을 살펴보\\n면, 띄어쓰기를 기준으로 벡터로 표현하기 위한 총 차원 수는\\n13이다. 1번 문장을 구성하는 “회의”, “중이니”, “나중에”, “하\\n장을 구성하지 않는 단어들은 0으로, 문장을 구성하는 단어들\\n그림 1. CBOW와 skip gram\\n겠습니다.”라는 단어들을 가지고 문장 벡터로 표현한다면, 문\\n2. SMS 문자 메시지의 띄어쓰기 오류 문제\\n은 각 단어의 빈도수로 채우는 형태의 [0, 1, 0, 0, 1, 1, 0, 0,\\n사용자들이 문자 메시지를 전송할 때 단어들 사이에 공백\\n1, 0, 1, 0, 0]와 같은 문장 벡터를 생성하게 된다. 하지만 이러\\n문자를 의도적으로 삽입하거나 제거하여 문자 메시지를 전송\\n한 문장 벡터는 단어 수가 증가함에 따라 차원 수도 증가하는\\n한다. 아래 예제는 스팸 문자 메시지에 자동 띄어쓰기를 적용\\n문제점을 내포하고 있다.\\n1. 회의  중이니  나중에  전화하겠습니다.\\n2. 즐거운  주말보내세요\\n3. 지금  전화를  받을  수  없습니다.\\n4. 저녁  약속있나요?\\n5. 저녁먹자^^~~\\n한 예제의 일부이다.\\n- 자동 띄어쓰기 전:\\n년엄청혜택QQ2480.com\\n- 자동 띄어쓰기 후:\\n1억을진짜먹네남일같았던1억강원에뺏긴돈이곳에선가능신\\n이러한 문제점을 해결하기 위해, 차원 수 조절에 자유롭고,\\n가능신년 엄청혜택 QQ2480.com\\n단어의 의미를 벡터로 표현하는데도 효율적인 신경망을 이용\\n한 CBOW와 skip gram으로 단어 벡터를 생성하여 전체 단어\\n위 예제를 살펴보면, “1억을진짜먹네남일같았던1억강원에\\n1억을 진짜먹네 남일 같았던 1억 강원에 뺏긴돈 이곳에선\\n\\n\\n\\n\\n\\nPage 3\\n뺏긴돈이곳에선가능신년엄청혜택QQ2480.com”의 문장은 띄\\n- 보험 가입 후 나는 늦은 밤에 숙소에서 저녁밥을 먹었다.\\n어쓰기를 기준으로 문장 자체가 하나의 단어인 희소한 형태로\\n이루어진다. 이러한 희소한 형태의 단어는 워드 임베딩에 의\\n스팸 문자 메시지에서는 “첫!가!입!”, “㉸㉨ⓛN0”, “ㅇ‡ㅁ†토\\n한 문장 및 구를 벡터로 표현하는 질을 저하 시키는 요인이 될\\n가”, “┓┠입 ”, “つг입즉ΛΙ”와 같은 한글단어들의 초성, 중성,\\n수 있다[13].\\n종성을 유사한 형태의 영어나 다른 나라의 언어, 특수기호, 숫자\\n등으로 대치하여 단어를 다양한 형태에 단어로 왜곡시킨다. 그\\n- 공백이 삽입된 단어의 자동 띄어쓰기 예\\n리하여 보편적인 스팸 문자 메시지 필터링 시스템에서는 이러\\n띄어쓰기 전 : 띄어쓰기 후\\n한 왜곡된 단어들을 “첫가입”, “카지노”, “가입즉시”와 같은 정\\n애 플\\n:\\n애플\\nＮ Ｈ 금 융 :\\nＮＨ금융\\n대 리 운 전 :\\n대리운전\\n규화 과정을 거처 스팸 문자 메시지 필터링을 위한 어휘 사전과\\n의 비교를 통해 스팸 문자 메시지를 차단하고 있다[18, 19]. 분\\n산 가설에 의한 워드 임베딩은 왜곡된 단어의 문맥과 왜곡되지\\n않은 단어의 문맥이 유사하다면 같은 문맥을 가진 단어는 벡터\\n위 예제는 문자 메시지의 한 어절마다 공백문자를 삽입하여\\n공간상에서 유사한 위치에 나타나도록 단어 벡터를 위치시킨다.\\n단어를 변형시킨 스팸 문자메시지의 예제 일부이다. 위 예제\\nCBOW와 skip gram을 통한 워드 임베딩에서는 “가입즉시”와\\n를 살펴보면 하나의 어절마다 띄어쓰기를 적용하여 “N H 금\\n“つг입즉ΛΙ”의 각 단어 벡터는 같은 문맥과 함께 사용된다면 벡\\n융” 등과 같이 고의로 공백문자를 삽입하여 “NH금융”의 단어\\n터 공간상에 유사한 위치에 분포한다. 이러한 분포를 통해 두\\n를 변형시킨다. 이러한 변형된 형태는 의미가 유사한 단어는\\n단어는 유사한 의미를 지닌 단어라고 유추가 가능하고, 또한 왜\\n그 단어의 주변의 단어들도 유사한 분포를 가진다는 분산 가\\n곡된 단어의 문맥과 왜곡 되지 않은 단어의 문맥이 다르면 각\\n설(distribution hypothesis)에 의한 워드 임베딩의 단어를 표\\n단어는 벡터 공간상에서 유사한 위치가 아닌 먼 거리에 위치하\\n현하는 벡터의 질을 저하시킨다. 즉, “애플”이라는 단어 벡터\\n여 각 단어를 구분하는데 용이하다. 이를 통해 자모를 특수한\\n생성 시 자동 띄어쓰기를 적용하지 않는 경우 “애”와 “플”이\\n기호로 대치한 왜곡된 형태의 단어의 정규화를 거친 단어들이\\n라는 각각 음절이 서로를 의미를 나타내는 문맥(context)으로\\n포함된 정상적인 문자 메시지를 스팸 문자 메시지로 잘 못 필터\\n사용되어 저 빈도 음절 벡터를 생성한다. 그리하여 “애플”이\\n링하는 경우를 예방하고자 하였고, 왜곡된 단어를 정규화하는\\n라는 단어 벡터 생성 시 필요한 문맥에 영향을 주어 올바른 단\\n불필요한 과정을 생략하였다. 이러한 왜곡된 단어들도 하나의\\n어 벡터 생성에 영향을 미친다[15, 16].\\n단어 토큰(token)으로 하는 방법을 통해 왜곡되지 않은 단어와\\n본 연구에서는 분산 가설에 의해 자동 띄어쓰기를 적용하여\\n왜곡된 단어의 구분하기 좋은 단어 벡터들을 생성하였다.\\n어절마다 공백이 삽입된 단어는 공백을 제거하고, 공백을 제\\n문자 메시지는 이를 구성하는 단어들의 벡터 합으로 문장 벡\\n거한 문장은 공백을 삽입하여 올바른 형태의 단어와 문장으로\\n터로 표현하였다. 이는 유사한 단어들의 집합으로 이루어진 문\\n바꾸는 과정을 수행하여 단어의 의미를 잘 표현하는 단어 벡\\n자 메시지는 단어 집합에 속한 단어들의 벡터 합으로 벡터 공간\\n터를 생성하였다[17].\\n상에 유사한 위치로 위치시킬 수 있다. 그리하여 문자 메시지를\\n구성하는 단어들의 벡터 합으로 문장 및 구를 벡터로 표현하였\\n본 연구에서는 분류기로 자질 벡터의 구성이 분류 성능에 영\\n3. 문장 벡터를 이용한 스팸 문자 메시지 필터링\\n다.\\n분산 가설에 의한 워드 임베딩은 유사한 의미를 가진 단어들\\n향을 큰 영향을 미치는 지지 벡터 기계보다 벡터 외에도 은닉\\n은 벡터 공간상에 유사한 위치에 위치시킨다. 이를 위하여 전처\\n층(hidden layer)의 수, 활성화 함수(activation function), 학\\n리 과정으로서 자동 띄어쓰기를 적용하고 문장을 구성하는 단\\n습 알고리즘 등의 요인들을 통해 분류 성능에 영향을 끼치는 전\\n어의 분포를 올바르게 유지하여 최대한 유사한 문맥들과 위치\\n방향 신경망을 분류기로 사용하여 문자 메시지를 스팸 문자 메\\n한 단어들은 벡터 공간상에서 유사한 위치에 위치하도록 하였\\n시지 또는 정상적인 문자 메시지로 분류하였다.\\n다. 아래 예제를 분산 가설로 살펴보면 “호텔에서”와 “숙소에\\n그림 2의 분류(classification)과정은 전방향 신경망에 의한\\n서”의 단어 의미는 문맥인 “보험”, “가입”, “후”, “나는”, “늦은”,\\n이진 분류이며 전방향 신경망의 비선형 함수와 비용(cost) 함수\\n“밤에”, “저녁밥을”, “먹었다”에 의해 표현되어 벡터 공간 상에\\n로는 시그모이드(sigmoid)\\n함수,\\n교차 엔트로피\\n유사한 위치에 나타난다[17].\\n(cross-entropy) 함수를 이용하였다.\\n- 보험 가입 후 나는 늦은 밤에 호텔에서 저녁밥을 먹었다.\\n\\nPage 4\\n문자 메시지를 구성하는 문장 및 구에 대한 벡터 표현을 위\\n해 CBOW와 skip gram이라는 두 가지 방식의 워드 임베딩 알\\n고리즘을 이용하여 단어 벡터를 생성하는 실험을 수행하였다.\\n각각 워드 임베딩 방식을 통하여 하나의 문자 메시지를 구성\\n하는 단어들의 벡터 합으로 문자 메시지를 벡터로 표현하였\\n고, 이러한 고정 길의 문장 벡터로 딥러닝 모델인 전방향 신경\\n망과 SVM Light를 이용하여 스팸 문자 메시지 필터링의 정\\n확도를 비교 측정하였다. 표 2는 전방향 신경망과 SVM Light\\n에 의한 스팸 문자 메시지 필터링의 정확도 측정 결과이다. 전\\n방향 신경망의 학습 방법은 경사도 하강법(gradient descent)\\n을 이용하여 학습하였다.\\n표 2. SVM Light와 전방향 신겸망의 정확도\\n그림 2. 워드 임베딩과 딥러닝을 이용한 문자 메시지\\n필터링 과정\\n교차 엔트로피 함수의 입력 값으로는 스팸 문자 메시지인\\n지 정상적인 문자 메시지인지를 나타내는 2차원 벡터 형태의\\n정답 레이블(label)과 전방향 신경망의 출력 층의 2차원 벡터\\n값을 소프트 맥스(softmax) 함수로 정규화한 값으로 이 두 개\\nSVM  Light\\nCBOW\\nSkip  gram\\nCBOW\\nFeed-forward\\nNeural  Network\\n2차원 벡터 값들의 확률분포 비교하여 교차 엔트로피 함수를\\nSkip  gram\\n최소화하는 방향으로 학습을 진행하여 스팸 문자 메시지를 필\\nLayer\\n⋅\\n⋅\\n1\\n2\\n3\\n4\\n1\\n2\\n3\\n4\\nAccuracy(%)\\n95.72\\n93.57\\n95.4\\n95.19\\n95.87\\n95.53\\n93.56\\n93.84\\n93.57\\n94.06\\n터링하였다.\\n4. 실험 및 성능 평가\\n워드 임베딩에서 윈도우 크기는 8로 하였고, 워드 벡터는\\n300차원으로 구성하였다. 표 2의 스팸 문자 메시지는 스팸 문\\n자 메시지로 정상적인 문자 메시지는 정상적인 문자 메시지로\\n워드 임베딩과 전방향 신경망을 이용한 스팸 문자 메시지\\n올바르게 분류하는 정확도에서 딥러닝 모델인 전방향 신경망\\n필터링을 위한 실험 데이터는 총 109,993개의 문자 데이터 집\\n을 이용한 경우는 95.87%가 가장 높은 정확도로 SVM light의\\n합이다. 이 데이터는 중복 단어를 포함하여 910,143개 단어들\\n95.72%보다 0.15% 더 높은 성능을 보여주고 있다.\\n로 구성되었고 중복 단어 제거하면 895,918개의 단어들로 이\\n루어졌다. 실험 데이터를 표 1과 같이 학습 및 테스트용으로\\n분할하였다.\\n표 3. CBOW와 Skip gram의 차원 수에 따른 스팸 문자\\n메시지 필터링의 정확도\\n표 1. 문자 메시지의 데이터 수\\nLines\\n5,000\\n5,000\\n49,993\\n50,000\\n109,993\\nTest\\nTrain\\nHam\\nSpam\\nHam\\nSpam\\nTotal\\nWords\\n(with \\nduplication)\\n27,785\\n44,642\\n413,441\\n424,275\\n910,143\\nWords\\n(without \\nduplication)\\n27,732\\n44,610\\n403,520\\n420,056\\n895,918\\nHam  :  정상적인  문자  메시지\\nSpam  :  스팸  문자  메시지\\nSVM  Light\\nCBOW\\nSkip  gram\\nDimension Accuracy(%)\\n200\\n250\\n300\\n200\\n250\\n300\\n93.84\\n95.65\\n95.72\\n93.78\\n93.43\\n93.57\\n표 3의 SVM light를 사용하여 CBOW와 skip gram의 벡터\\n차원 수에 따른 정상적인 문자 메시지는 정상적인 문자메시지\\n로 스팸 문자메시지는 스팸 문자메시지로 올바르게 분류하는\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPage 5\\n정확도에서는 CBOW과 skip gram보다 더 높은 성능을 보여\\n주고 있다. 이를 통해, 단어를 벡터로 표현하는 방법에서는\\nskip gram보다 CBOW를 이용하는 것이 정확도가 더 높은 것\\n을 알 수 있었다.\\nⅢ. 결 론\\n핸드폰의 SMS 문자 메시지로 전송되는 대량의 광고문자들\\n을 필터링하는 방법으로 워드 임베딩과 딥러닝 기법을 이용한\\n스팸 문자 메시지 필터링 방법을 제안하였다. 스팸 문자들은\\n의도적으로 띄어쓰기 규칙을 지키지 않거나 한글 자모 대신에\\n특수기호나 기호를 이용하여 음절을 구성하는 경우가 많은데\\n자동 띄어쓰기를 이용하여 띄어쓰기 오류를 교정하고, 자모\\n해체와 특수기호로 대치된 경우는 왜곡된 단어 형태를 포함하\\n여 워드 임베딩을 수행하였다. 문자 메시지를 구성하는 문장\\n벡터 표현은 CBOW와 skip gram이라는 두 가지 방식으로 워\\n드 임베딩 실험을 수행하였고, 딥러닝 기법의 효용성을 검증\\n하기 위해 SVM 방법의 문자 필터링과 성능을 비교하였다. 그\\n결과로 SVM과 딥러닝 기법 모두 CBOW를 이용한 워드 임베\\n딩 성능이 skip gram 방식보다 정확도가 높은 것을 알 수 있\\n었고, 또한 딥러닝 기법이 SVM 방식보다 더 성능이 좋은 것\\n을 확인하였다.\\nREFERENCES\\n[1] 박경민, 최훈, 이창건, 황인태, 이칠우, \"휴대 단말\\n을 위한 지능형 사용자 인터페이스 플랫폼,\" 스마\\n트미디어저널, 제1권, 제4호, 44-51쪽, 2012년 12월\\n[2] 손대능, 이정태, 이승욱, 신중휘, 임해창, “문자 메\\n시지의 특성을 고려한 한국어 모바일 스팸필터링\\n시스템,” 한국산학기술학회논문지, 제11권, 제7호,\\n2595-2602쪽, 2010년 7월\\n[3] M. Salib, “MeatSlicer: Spam Classification with\\nNaive Bayes and Smart Heuristics,” Proceedings\\nofthe SpamConference, Dec. 2002.\\n[4] K. Schneider, “A Comparison of Event Models\\nfor Naive Bayes Anti-Spam E-Mail Filtering,”\\nProceedings of10thConference ofthe European\\nChapter of the Association for Computational\\nLinguistics(EACL 2003), Budapest, Hungary, vol.\\n1, pp. 307-314, April. 2003.\\n[5] 강승식, “메일 주소 유효성과 제목-내용 가중치 기\\n법에 의한 스팸 메일 필터링,” 멀티미디어학회 논\\n문지, 제9권, 제2호, 255-263쪽, 2006년 2월\\n[6] Drucker, H., Wu, D., & Vapnik, V. N., “Support\\nfor Spam Categorization,”\\nVector Machines\\nIEEE Transactions on Neural networks, vol. 10,\\nIssue 5, pp. 1048-1054, Sep. 1999\\n[7] 허기수, 정현태, 박아론, 백성준, \"양자 유전 알고\\n리즘을 이용한 특징 선택 및 성능 분석,\" 스마트미\\n디어저널, 제1권, 제1호, 40-45쪽, 2012년 3월\\n[8] Mikolov, T., Sutskever, I., Chen, K., Corrado, G.\\nS., & Dean, J., “Distributed Representations of\\nWords and Phrases and their Compositionality,”\\nIn Advances in neural information processing\\nsystems, Lake Tahoe,\\nthe United States, pp.\\n3111-3119, Dec. 2013.\\n[9] Mikolov, Tomáš,\\net\\nal.,\\nbased\\n\"Recurrent neural\\nlanguage model,\" Eleventh\\nnetwork\\nAnnual Conference of the International Speech\\nCommunication Association, Makuhari, Chiba,\\nJapan, pp. 1045-1048, Sep. 2010.\\n[10] Mikolov, T., Yih, W. T., & Zweig, G.,\\nin Continuous Space\\n“Linguistic Regularities\\nWord Representations,” In Proceedings of the\\n2013 Conference ofthe NorthAmerican Chapter\\nofthe Association for Computational Linguistics:\\nHumanLanguage Technologies, Atlanta, Georgia\\n, the United States, pp. 746-751, Jun. 2013.\\n[11] Manevitz, L. M., & Yousef, M., “One-class\\nSVMs for Document Classification,” Journal of\\nmachine Learning research, vol. 2, pp. 139-154,\\nDec. 2001.\\n“Parsing Natural Scenes\\n[12] Socher, R., Lin, C. C., Manning, C., & Ng, A.\\nY.,\\nand Natural\\nLanguage with Recursive Neural Networks,” In\\nProceedings ofthe 28th international conference\\non machine learning (ICML-11), Bellevue,\\nWashington, USA, pp. 129-136, Jul. 2011.\\n[13] Chen, D., & Manning, C., “A Fast and Accurate\\nDependency Parser using Neural Networks,” In\\nProceedings ofthe 2014 conference on empirical\\nmethods in natural language processing\\n(EMNLP), Doha, Qatar, pp. 740-750, Oct. 2014.\\n[14] Simard, P. Y., Steinkraus, D., & Platt, J. C.,\\n“Best\\nConvolutional Neural\\nfor\\nNetworks Applied to Visual Document Analysis,”\\nIn Proceedings of the 7th International\\nConference on Document Analysis and\\nRecognition(ICDAR 2003), Edinburgh, Scotland,\\nUK, vol. 2, pp. 958-962, Aug. 2003.\\nPractices\\n[15] Mikolov, Tomas, et al. “Efficient estimation of\\nword representations in vector space,” arXiv\\npreprint arXiv:1301.3781, 2013.\\n[16] Sahlgren, M., “The distributional hypothesis,”\\nItalian Journal of Disability Studies, vol.20, pp.\\n\\nPage 6\\n33-53, 2008.\\n[17] 강승식, \"음절 bigram을 이용한 띄어쓰기 오류의\\n자동 교정,\" 음성과학, 제8권, 제2호, 83-90쪽, 2001\\n년 6월\\n[18] 강승식, 장두성, \"SMS 변형된 문자열의 자동 오\\n류 교정 시스템,\" 정보과학회논문지, 제35권, 제6\\n호, 386-391쪽, 2008년 6월\\n[19] 강승식, \"스팸 문자 필터링을 위한 변형된 한글\\nSMS 문장의 정규화 기법,\" 정보처리학회논문지,\\n제3권, 제7호, 271-276쪽, 2014년 7월\\n저 자 소 개\\n이현영(학생회원)\\n2016년 국민대학교 컴퓨터공학 학과\\n학사 졸업(공학사).\\n2016년 ~ 2017년 SK hynix memory\\nsolutions inc. Intern\\n2017년 ~ 현재 국민대학교 컴퓨터공\\n학 학과 석사과정.\\n사진\\n사진\\n23x30\\n<주관심분야 : 자연어처리, 머신 러닝, 딥러닝, 빅데이\\n터 분석>\\n23x30\\n강승식(정회원)\\n1986년 서울대학교 전자계산기공학과\\n1988년 서울대학교 전자계산기공학과\\n1993년 서울대학교 전자계산기공학과\\n학사 졸업.\\n학과 석사 졸업.\\n학과 박사 졸업.\\n이닝, 빅데이터 분석, 상황인지 컴퓨팅>\\n<주관심분야 : 자연어처리, 텍스트 마\\n\\n\\n\\n\\nPage: 1, 2, 3, 4, 5, 6\\n\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Page 1 워드 임베딩과 딥러닝 기법을 이용한 SMS 문자 메시지 필터링  SMS Text Messages Filtering using Word Embedding and Deep Learning Techniques  이현영  강승식 Hyun Young Lee  Seung Shik Kang 딥러닝에서 자연어 처리를 위한 텍스트 분석 기법은 워드 임베딩을 통해 단어를 벡터 형태로 표현한다  본 논문에서는 워드 임베딩 기법과 딥러닝 기법을 이용하여 SMS 문자 메시지를 문서 벡터로 구성하고 이를 스팸 문자 메시지와 정상적 인 문자 메시지로 분류하는 방법을 제안하였다  유사한 문맥을 가진 단어들은 벡터 공간에서 인접한 벡터 공간에 표현되 도록 하기 위해 전처리 과정으로 자동 띄어쓰기를 적용하고 스팸 문자 메시지로 차단되는 것을 피하기 위한 목적으로 음 절의 자모를 특수기호로 왜곡하여 맞춤법이 파괴된 상태로 단어 벡터와 문장 벡터를 생성하였다  또한 문장 벡터 생성 시 CBOW와 skip gram이라는 두 가지 워드 임베딩 알고리즘을 적용하여 문장 벡터를 표현하였으며  딥러닝을 이용한 스팸 문자 메시지 필터링의 성능 평가를 위해 SVM Light와 정확도를 비교 측정하였다  중심어   스팸 문자 메시지   워드 임베딩   문장 벡터   딥러닝   이진 분류 Text analysis technique for natural language processing in deep learning represents words in vector form through word embedding  In this paper  we propose a method of constructing a document vector and classifying it into spam and normal text message  using word embedding and deep learning method Automatic spacing applied in the preprocessing process ensures that words with similar context are adjacently represented in vector space  Additionally the intentional word formation errors with non alphabetic or extraordinary characters are designed to avoid being blocked by spam message filter Two embedding algorithms  CBOW and skip grams  are used to produce the sentence vector and the performance and the accuracy of deep learning based spam filter model are measured by comparing to those of SVM Light  keywords  spam text message   word embedding   text vector   deep learning   binary classification  서 론 이터를 분류하는 것에 있어서 다른 기법들보다 더 나은 성능을 보인다고 알려져 있다 5  6  지지 벡터 기계는 자질 벡터 feature vector 를 이용하는 기계학습 방법으로 자질 벡터의 스마트폰이 대중화되면서 SMS short message service  문 구성이 분류 성능에 큰 영향을 미친다  이에 따라 스팸 문자 메 자 메시지 전송을 통한 의사소통 방식이 보편화됨에 따라 텍스 시지 필터링을 위한 벡터 표현은 성능에 영향을 끼치는 중요한 트 형태의 데이터의 양도 대량으로 증가하고 있다 1  그 부작 요소이다 7  현재 텍스트에 대한 벡터의 표현 방식으로 용으로 성인광고  대출광고  게임광고 등의 스팸 문자 텍스트 데 TF IDF가 가장 널리 사용되고 있지만  TF IDF의 경우에는 이터도 폭발적으로 증가하여 이를 필터링 하기 위한 다양한 기 단어 수가 증가함에 따라 차원의 수도 같이 증가하여 대용량 단 법들이 개발되고 있다 2  이러한 스팸 문자 메시지 데이터를 어들을 처리하는데 어려움 있다  이러한 차원수 증가 문제를 해 효율적으로 필터링하는 방법 중 널리 보편적으로 사용되는 방 결하기 위해 TF IDF는 고차원의 벡터를 저차원의 벡터로 차 법론으로는 통계적 확률 방법론과 지지 벡터 기계 SVM  등의 원 축소 작업을 추가로 수행해야 한다  하지만 신경망 언어 모 기계학습 방법이 있다 3  4 델의 워드 임베딩 통한 단어 벡터는 기존의 TF IDF보다 차원 기계학습 방법론 중  지지 벡터 기계는 스팸 문자 메시지 데 축소에 자유롭고  벡터를 통한 단어 의미 유사성 추론에서도 더  학생회원  국민대학교 컴퓨터공학학과  정회원  국민대학교 소프트웨어학부 이 논문은 2017년도 정부 미래창조과학부 의 재원으로 한국연구재단의 지원을 받아 수행된 연구임  No NRF 2017M3C4A7068186 접수일자   2018년 03월 05일 수정일자   2018년 04월 11일 게재확정일   2018년 04월 28일 교신저자   강승식 e mail   sskang kookmin ac kr Page 2 나은 성능을 보여준다 8  9  10 수에 비례하게 커지는 차원 수 증가 문제를 해결하였다 13 문서나 문장 등의 데이터를 이미 정해진 범주에 따라 분류 그림 1은 단어 벡터를 표현하는 방법인 CBOW와 skip 하는 방법으로는 K NN 방법  SVM  Naive Bayes 등의 기계 gram이다  이 두 개의 워드 임베딩 방법은 서로 다른 방법으 학습 방법들을 이용하고 있다 11  하지만 스팸 문자 메시지 로 단어를 벡터로 표현한다  CBOW의 경우는 중심 단어의 주 필터링은 정해진 범주를 2개로 고정하여 여러 문장 및 구 형 변 단어들로부터 중심단어를 예측하고  skip gram은 중심단 태의 텍스트 데이터를 분류하는 이진 분류 binary 어에서 주변 단어들 예측하여 중심 단어를 벡터로 표현한다 classification  문제이므로 분류 기법에 적합한 딥러닝 모델을 단어 벡터를 생성하는 CBOW와 skip gram으로 문장을 구성 이용하여 스팸 문자 메시지를 필터링하고자 하였다 하는 단어 벡터를 생성하여 문장 벡터로 표현하였다 딥러닝 모델 중 분류 문제에 가장 널리 사용되는 모델은 연 속성의 데이터를 분류하는 RNN recurrent neural network 특징 벡터를 추출하여 분류하는 CNN convolution neural network  고정 길이의 특징 벡터를 분류하는 전방향 신경망 feed forward neural network  등이 있다 12  13  14 본 연구에서는 신경망을 이용한 워드 임베딩 word embedding 으로 문자 메시지를 벡터로 표현하고 전방향 신경 망을 이용하는 스팸 문자 메시지 필터링 기법을 제안한다  본 론 1  워드 임베딩과 문장 벡터 구성 SMS 문자 메시지 데이터를 살펴보면  하나의 문장 및 구는 단어들의 집합이므로 각 단어들의 빈도수를 이용하여 하나의 벡터로 표현될 수 있다  이러한 빈도수 기반의 벡터는 단어의 수가 많아짐에 따라 차원의 수도 비례하게 커지는 차원의 저 주라는 문제점이 있다  예를 들어  아래 5개의 문장을 살펴보 면  띄어쓰기를 기준으로 벡터로 표현하기 위한 총 차원 수는 13이다  1번 문장을 구성하는  회의   중이니   나중에   하 장을 구성하지 않는 단어들은 0으로  문장을 구성하는 단어들 그림 1  CBOW와 skip gram 겠습니다 라는 단어들을 가지고 문장 벡터로 표현한다면  문 2  SMS 문자 메시지의 띄어쓰기 오류 문제 은 각 단어의 빈도수로 채우는 형태의  0  1  0  0  1  1  0  0 사용자들이 문자 메시지를 전송할 때 단어들 사이에 공백 1  0  1  0  0 와 같은 문장 벡터를 생성하게 된다  하지만 이러 문자를 의도적으로 삽입하거나 제거하여 문자 메시지를 전송 한 문장 벡터는 단어 수가 증가함에 따라 차원 수도 증가하는 한다  아래 예제는 스팸 문자 메시지에 자동 띄어쓰기를 적용 문제점을 내포하고 있다 1  회의  중이니  나중에  전화하겠습니다 2  즐거운  주말보내세요 3  지금  전화를  받을  수  없습니다 4  저녁  약속있나요 5  저녁먹자 한 예제의 일부이다  자동 띄어쓰기 전 년엄청혜택QQ2480 com  자동 띄어쓰기 후 1억을진짜먹네남일같았던1억강원에뺏긴돈이곳에선가능신 이러한 문제점을 해결하기 위해  차원 수 조절에 자유롭고 가능신년 엄청혜택 QQ2480 com 단어의 의미를 벡터로 표현하는데도 효율적인 신경망을 이용 한 CBOW와 skip gram으로 단어 벡터를 생성하여 전체 단어 위 예제를 살펴보면   1억을진짜먹네남일같았던1억강원에 1억을 진짜먹네 남일 같았던 1억 강원에 뺏긴돈 이곳에선 Page 3 뺏긴돈이곳에선가능신년엄청혜택QQ2480 com 의 문장은 띄  보험 가입 후 나는 늦은 밤에 숙소에서 저녁밥을 먹었다 어쓰기를 기준으로 문장 자체가 하나의 단어인 희소한 형태로 이루어진다  이러한 희소한 형태의 단어는 워드 임베딩에 의 스팸 문자 메시지에서는  첫 가 입   N0   ㅇ ㅁ 토 한 문장 및 구를 벡터로 표현하는 질을 저하 시키는 요인이 될 가   입    입즉 와 같은 한글단어들의 초성  중성 수 있다 13 종성을 유사한 형태의 영어나 다른 나라의 언어  특수기호  숫자 등으로 대치하여 단어를 다양한 형태에 단어로 왜곡시킨다  그  공백이 삽입된 단어의 자동 띄어쓰기 예 리하여 보편적인 스팸 문자 메시지 필터링 시스템에서는 이러 띄어쓰기 전   띄어쓰기 후 한 왜곡된 단어들을  첫가입   카지노   가입즉시 와 같은 정 애 플 애플    금 융  금융 대 리 운 전  대리운전 규화 과정을 거처 스팸 문자 메시지 필터링을 위한 어휘 사전과 의 비교를 통해 스팸 문자 메시지를 차단하고 있다 18  19  분 산 가설에 의한 워드 임베딩은 왜곡된 단어의 문맥과 왜곡되지 않은 단어의 문맥이 유사하다면 같은 문맥을 가진 단어는 벡터 위 예제는 문자 메시지의 한 어절마다 공백문자를 삽입하여 공간상에서 유사한 위치에 나타나도록 단어 벡터를 위치시킨다 단어를 변형시킨 스팸 문자메시지의 예제 일부이다  위 예제 CBOW와 skip gram을 통한 워드 임베딩에서는  가입즉시 와 를 살펴보면 하나의 어절마다 띄어쓰기를 적용하여  N H 금 입즉 의 각 단어 벡터는 같은 문맥과 함께 사용된다면 벡 융  등과 같이 고의로 공백문자를 삽입하여  NH금융 의 단어 터 공간상에 유사한 위치에 분포한다  이러한 분포를 통해 두 를 변형시킨다  이러한 변형된 형태는 의미가 유사한 단어는 단어는 유사한 의미를 지닌 단어라고 유추가 가능하고  또한 왜 그 단어의 주변의 단어들도 유사한 분포를 가진다는 분산 가 곡된 단어의 문맥과 왜곡 되지 않은 단어의 문맥이 다르면 각 설 distribution hypothesis 에 의한 워드 임베딩의 단어를 표 단어는 벡터 공간상에서 유사한 위치가 아닌 먼 거리에 위치하 현하는 벡터의 질을 저하시킨다  즉   애플 이라는 단어 벡터 여 각 단어를 구분하는데 용이하다  이를 통해 자모를 특수한 생성 시 자동 띄어쓰기를 적용하지 않는 경우  애 와  플 이 기호로 대치한 왜곡된 형태의 단어의 정규화를 거친 단어들이 라는 각각 음절이 서로를 의미를 나타내는 문맥 context 으로 포함된 정상적인 문자 메시지를 스팸 문자 메시지로 잘 못 필터 사용되어 저 빈도 음절 벡터를 생성한다  그리하여  애플 이 링하는 경우를 예방하고자 하였고  왜곡된 단어를 정규화하는 라는 단어 벡터 생성 시 필요한 문맥에 영향을 주어 올바른 단 불필요한 과정을 생략하였다  이러한 왜곡된 단어들도 하나의 어 벡터 생성에 영향을 미친다 15  16 단어 토큰 token 으로 하는 방법을 통해 왜곡되지 않은 단어와 본 연구에서는 분산 가설에 의해 자동 띄어쓰기를 적용하여 왜곡된 단어의 구분하기 좋은 단어 벡터들을 생성하였다 어절마다 공백이 삽입된 단어는 공백을 제거하고  공백을 제 문자 메시지는 이를 구성하는 단어들의 벡터 합으로 문장 벡 거한 문장은 공백을 삽입하여 올바른 형태의 단어와 문장으로 터로 표현하였다  이는 유사한 단어들의 집합으로 이루어진 문 바꾸는 과정을 수행하여 단어의 의미를 잘 표현하는 단어 벡 자 메시지는 단어 집합에 속한 단어들의 벡터 합으로 벡터 공간 터를 생성하였다 17 상에 유사한 위치로 위치시킬 수 있다  그리하여 문자 메시지를 구성하는 단어들의 벡터 합으로 문장 및 구를 벡터로 표현하였 본 연구에서는 분류기로 자질 벡터의 구성이 분류 성능에 영 3  문장 벡터를 이용한 스팸 문자 메시지 필터링 다 분산 가설에 의한 워드 임베딩은 유사한 의미를 가진 단어들 향을 큰 영향을 미치는 지지 벡터 기계보다 벡터 외에도 은닉 은 벡터 공간상에 유사한 위치에 위치시킨다  이를 위하여 전처 층 hidden layer 의 수  활성화 함수 activation function  학 리 과정으로서 자동 띄어쓰기를 적용하고 문장을 구성하는 단 습 알고리즘 등의 요인들을 통해 분류 성능에 영향을 끼치는 전 어의 분포를 올바르게 유지하여 최대한 유사한 문맥들과 위치 방향 신경망을 분류기로 사용하여 문자 메시지를 스팸 문자 메 한 단어들은 벡터 공간상에서 유사한 위치에 위치하도록 하였 시지 또는 정상적인 문자 메시지로 분류하였다 다  아래 예제를 분산 가설로 살펴보면  호텔에서 와  숙소에 그림 2의 분류 classification 과정은 전방향 신경망에 의한 서 의 단어 의미는 문맥인  보험   가입   후   나는   늦은 이진 분류이며 전방향 신경망의 비선형 함수와 비용 cost  함수 밤에   저녁밥을   먹었다 에 의해 표현되어 벡터 공간 상에 로는 시그모이드 sigmoid 함수 교차 엔트로피 유사한 위치에 나타난다 17 cross entropy  함수를 이용하였다  보험 가입 후 나는 늦은 밤에 호텔에서 저녁밥을 먹었다 Page 4 문자 메시지를 구성하는 문장 및 구에 대한 벡터 표현을 위 해 CBOW와 skip gram이라는 두 가지 방식의 워드 임베딩 알 고리즘을 이용하여 단어 벡터를 생성하는 실험을 수행하였다 각각 워드 임베딩 방식을 통하여 하나의 문자 메시지를 구성 하는 단어들의 벡터 합으로 문자 메시지를 벡터로 표현하였 고  이러한 고정 길의 문장 벡터로 딥러닝 모델인 전방향 신경 망과 SVM Light를 이용하여 스팸 문자 메시지 필터링의 정 확도를 비교 측정하였다  표 2는 전방향 신경망과 SVM Light 에 의한 스팸 문자 메시지 필터링의 정확도 측정 결과이다  전 방향 신경망의 학습 방법은 경사도 하강법 gradient descent 을 이용하여 학습하였다 표 2  SVM Light와 전방향 신겸망의 정확도 그림 2  워드 임베딩과 딥러닝을 이용한 문자 메시지 필터링 과정 교차 엔트로피 함수의 입력 값으로는 스팸 문자 메시지인 지 정상적인 문자 메시지인지를 나타내는 2차원 벡터 형태의 정답 레이블 label 과 전방향 신경망의 출력 층의 2차원 벡터 값을 소프트 맥스 softmax  함수로 정규화한 값으로 이 두 개 SVM  Light CBOW Skip  gram CBOW Feed forward Neural  Network 2차원 벡터 값들의 확률분포 비교하여 교차 엔트로피 함수를 Skip  gram 최소화하는 방향으로 학습을 진행하여 스팸 문자 메시지를 필 Layer 1 2 3 4 1 2 3 4 Accuracy 95 72 93 57 95 4 95 19 95 87 95 53 93 56 93 84 93 57 94 06 터링하였다 4  실험 및 성능 평가 워드 임베딩에서 윈도우 크기는 8로 하였고  워드 벡터는 300차원으로 구성하였다  표 2의 스팸 문자 메시지는 스팸 문 자 메시지로 정상적인 문자 메시지는 정상적인 문자 메시지로 워드 임베딩과 전방향 신경망을 이용한 스팸 문자 메시지 올바르게 분류하는 정확도에서 딥러닝 모델인 전방향 신경망 필터링을 위한 실험 데이터는 총 109 993개의 문자 데이터 집 을 이용한 경우는 95 87 가 가장 높은 정확도로 SVM light의 합이다  이 데이터는 중복 단어를 포함하여 910 143개 단어들 95 72 보다 0 15  더 높은 성능을 보여주고 있다 로 구성되었고 중복 단어 제거하면 895 918개의 단어들로 이 루어졌다  실험 데이터를 표 1과 같이 학습 및 테스트용으로 분할하였다 표 3  CBOW와 Skip gram의 차원 수에 따른 스팸 문자 메시지 필터링의 정확도 표 1  문자 메시지의 데이터 수 Lines 5 000 5 000 49 993 50 000 109 993 Test Train Ham Spam Ham Spam Total Words with  duplication 27 785 44 642 413 441 424 275 910 143 Words without  duplication 27 732 44 610 403 520 420 056 895 918 Ham     정상적인  문자  메시지 Spam     스팸  문자  메시지 SVM  Light CBOW Skip  gram Dimension Accuracy 200 250 300 200 250 300 93 84 95 65 95 72 93 78 93 43 93 57 표 3의 SVM light를 사용하여 CBOW와 skip gram의 벡터 차원 수에 따른 정상적인 문자 메시지는 정상적인 문자메시지 로 스팸 문자메시지는 스팸 문자메시지로 올바르게 분류하는 Page 5 정확도에서는 CBOW과 skip gram보다 더 높은 성능을 보여 주고 있다  이를 통해  단어를 벡터로 표현하는 방법에서는 skip gram보다 CBOW를 이용하는 것이 정확도가 더 높은 것 을 알 수 있었다  결 론 핸드폰의 SMS 문자 메시지로 전송되는 대량의 광고문자들 을 필터링하는 방법으로 워드 임베딩과 딥러닝 기법을 이용한 스팸 문자 메시지 필터링 방법을 제안하였다  스팸 문자들은 의도적으로 띄어쓰기 규칙을 지키지 않거나 한글 자모 대신에 특수기호나 기호를 이용하여 음절을 구성하는 경우가 많은데 자동 띄어쓰기를 이용하여 띄어쓰기 오류를 교정하고  자모 해체와 특수기호로 대치된 경우는 왜곡된 단어 형태를 포함하 여 워드 임베딩을 수행하였다  문자 메시지를 구성하는 문장 벡터 표현은 CBOW와 skip gram이라는 두 가지 방식으로 워 드 임베딩 실험을 수행하였고  딥러닝 기법의 효용성을 검증 하기 위해 SVM 방법의 문자 필터링과 성능을 비교하였다  그 결과로 SVM과 딥러닝 기법 모두 CBOW를 이용한 워드 임베 딩 성능이 skip gram 방식보다 정확도가 높은 것을 알 수 있 었고  또한 딥러닝 기법이 SVM 방식보다 더 성능이 좋은 것 을 확인하였다 REFERENCES 1  박경민  최훈  이창건  황인태  이칠우   휴대 단말 을 위한 지능형 사용자 인터페이스 플랫폼  스마 트미디어저널  제1권  제4호  44 51쪽  2012년 12월 2  손대능  이정태  이승욱  신중휘  임해창   문자 메 시지의 특성을 고려한 한국어 모바일 스팸필터링 시스템  한국산학기술학회논문지  제11권  제7호 2595 2602쪽  2010년 7월 3  M  Salib   MeatSlicer  Spam Classification with Naive Bayes and Smart Heuristics  Proceedings ofthe SpamConference  Dec  2002 4  K  Schneider   A Comparison of Event Models for Naive Bayes Anti Spam E Mail Filtering Proceedings of10thConference ofthe European Chapter of the Association for Computational Linguistics EACL 2003  Budapest  Hungary  vol 1  pp  307 314  April  2003 5  강승식   메일 주소 유효성과 제목 내용 가중치 기 법에 의한 스팸 메일 필터링  멀티미디어학회 논 문지  제9권  제2호  255 263쪽  2006년 2월 6  Drucker  H  Wu  D    Vapnik  V  N   Support for Spam Categorization Vector Machines IEEE Transactions on Neural networks  vol  10 Issue 5  pp  1048 1054  Sep  1999 7  허기수  정현태  박아론  백성준   양자 유전 알고 리즘을 이용한 특징 선택 및 성능 분석  스마트미 디어저널  제1권  제1호  40 45쪽  2012년 3월 8  Mikolov  T  Sutskever  I  Chen  K  Corrado  G S    Dean  J   Distributed Representations of Words and Phrases and their Compositionality In Advances in neural information processing systems  Lake Tahoe the United States  pp 3111 3119  Dec  2013 9  Mikolov  Tom et al based Recurrent neural language model  Eleventh network Annual Conference of the International Speech Communication Association  Makuhari  Chiba Japan  pp  1045 1048  Sep  2010 10  Mikolov  T  Yih  W  T    Zweig  G in Continuous Space Linguistic Regularities Word Representations  In Proceedings of the 2013 Conference ofthe NorthAmerican Chapter ofthe Association for Computational Linguistics HumanLanguage Technologies  Atlanta  Georgia  the United States  pp  746 751  Jun  2013 11  Manevitz  L  M    Yousef  M   One class SVMs for Document Classification  Journal of machine Learning research  vol  2  pp  139 154 Dec  2001 Parsing Natural Scenes 12  Socher  R  Lin  C  C  Manning  C    Ng  A Y and Natural Language with Recursive Neural Networks  In Proceedings ofthe 28th international conference on machine learning  ICML 11  Bellevue Washington  USA  pp  129 136  Jul  2011 13  Chen  D    Manning  C   A Fast and Accurate Dependency Parser using Neural Networks  In Proceedings ofthe 2014 conference on empirical methods in natural language processing EMNLP  Doha  Qatar  pp  740 750  Oct  2014 14  Simard  P  Y  Steinkraus  D    Platt  J  C Best Convolutional Neural for Networks Applied to Visual Document Analysis In Proceedings of the 7th International Conference on Document Analysis and Recognition ICDAR 2003  Edinburgh  Scotland UK  vol  2  pp  958 962  Aug  2003 Practices 15  Mikolov  Tomas  et al   Efficient estimation of word representations in vector space  arXiv preprint arXiv 1301 3781  2013 16  Sahlgren  M   The distributional hypothesis Italian Journal of Disability Studies  vol 20  pp Page 6 33 53  2008 17  강승식   음절 bigram을 이용한 띄어쓰기 오류의 자동 교정  음성과학  제8권  제2호  83 90쪽  2001 년 6월 18  강승식  장두성   SMS 변형된 문자열의 자동 오 류 교정 시스템  정보과학회논문지  제35권  제6 호  386 391쪽  2008년 6월 19  강승식   스팸 문자 필터링을 위한 변형된 한글 SMS 문장의 정규화 기법  정보처리학회논문지 제3권  제7호  271 276쪽  2014년 7월 저 자 소 개 이현영 학생회원 2016년 국민대학교 컴퓨터공학 학과 학사 졸업 공학사 2016년   2017년 SK hynix memory solutions inc  Intern 2017년   현재 국민대학교 컴퓨터공 학 학과 석사과정 사진 사진 23x30 주관심분야   자연어처리  머신 러닝  딥러닝  빅데이 터 분석 23x30 강승식 정회원 1986년 서울대학교 전자계산기공학과 1988년 서울대학교 전자계산기공학과 1993년 서울대학교 전자계산기공학과 학사 졸업 학과 석사 졸업 학과 박사 졸업 이닝  빅데이터 분석  상황인지 컴퓨팅 주관심분야   자연어처리  텍스트 마 Page  1  2  3  4  5  6 '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "escape = re.compile('[^ a-zA-Z0-9\\u3131-\\u3163\\uac00-\\ud7a3]+') # 유니코드의 범위로 \n",
    "text = escape.sub(' ', text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
